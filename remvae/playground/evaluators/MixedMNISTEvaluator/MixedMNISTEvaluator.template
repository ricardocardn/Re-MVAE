def evaluate_mnist_accuracy(image_model, text_model, loader, tokenizer, device, results_dir):
    print("\nEvaluating Mixed MNIST Accuracy...")
    evaluator = MixedMNISTEvaluator(
        image_model, text_model, loader, tokenizer, device=device
    )
    
    image_to_text_acc, text_to_image_acc = evaluator.evaluate()

    print("\n--- Image-to-Text Accuracy ---")
    print(f"Top-1 Accuracy: {round(image_to_text_acc[0] * 100, 2)}%")
    print(f"Top-2 Accuracy: {round(image_to_text_acc[1] * 100, 2)}%")
    print(f"Top-3 Accuracy: {round(image_to_text_acc[2] * 100, 2)}%")

    print("\n--- Text-to-Image Accuracy ---")
    print(f"Top-1 Accuracy: {round(text_to_image_acc[0] * 100, 2)}%")
    print(f"Top-2 Accuracy: {round(text_to_image_acc[1] * 100, 2)}%")
    print(f"Top-3 Accuracy: {round(text_to_image_acc[2] * 100, 2)}%")

    save_metrics({
        "image_to_text_accuracy_top1": image_to_text_acc[0],
        "image_to_text_accuracy_top2": image_to_text_acc[1],
        "image_to_text_accuracy_top3": image_to_text_acc[2],
        "text_to_image_accuracy_top1": text_to_image_acc[0],
        "text_to_image_accuracy_top2": text_to_image_acc[1],
        "text_to_image_accuracy_top3": text_to_image_acc[2],
    }, results_dir, "eval_mnist_accuracy.json")