import os
import json
import torch
from torch.utils.data import DataLoader
from torchvision import transforms

from playground.readers.{{ reader }}.reader import Reader
from playground.architectures.{{ image_architecture }} import Builder as ImageBuilder
from playground.architectures.{{ text_architecture }} import Builder as TextBuilder, Wrapper
from playground.evaluators import (
    {% for evaluator in evaluators %}
    {{ evaluator }}{% if not loop.last %},{% endif %}
    {% endfor %}
)
from playground.helpers.tokenizer import TextTokenizer

{{ libs }}


def load_models_and_data(args_path):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    with open(args_path, 'r') as f:
        args = json.load(f)

    tokenizer = TextTokenizer.load(args["tokenizer_path"])
    transform = transforms.Compose([
        transforms.Resize((args["image_size"], args["image_size"])),
        transforms.ToTensor()
    ])

    dataset = Reader(train=False, transform=transform, len=args["dataset_length"])
    dataset.tokenizer = tokenizer
    loader = DataLoader(dataset, batch_size=1, shuffle=False)

    {{ image_model_init | indent(4) }}
    {{ text_model_init | indent(4) }}

    wrapper = Wrapper(text_model)

    checkpoint_base = args["checkpoint_dir"]
    dirs = [os.path.join(checkpoint_base, d) for d in os.listdir(checkpoint_base)]
    dirs = [d for d in dirs if os.path.isdir(d)]

    checkpoint_path = sorted(
        dirs,
        key=lambda x: os.path.getmtime(x),
        reverse=True
    )[0]

    image_model.load_state_dict(torch.load(
        os.path.join(checkpoint_path, 'image_model.pth'),
        map_location=device
    ))

    text_model.load_state_dict(torch.load(
        os.path.join(checkpoint_path, 'language_model.pth'),
        map_location=device
    ))

    image_model.to(device).eval()
    text_model.to(device).eval()

    return args, image_model, text_model, dataset, loader, device


def save_metrics(metrics, results_dir, filename):
    metrics_dir = os.path.join(results_dir, "metrics")
    os.makedirs(metrics_dir, exist_ok=True)
    path = os.path.join(metrics_dir, filename)
    with open(path, 'w') as f:
        json.dump(metrics, f, indent=4)
    print(f"Saved metrics to {path}")


{% for evaluator in evaluators %}
{% include 'generator/templates/evaluators/' + evaluator + '.template' %}


{% endfor %}


def main():
    args_path = "args.json"
    args, image_model, text_model, dataset, loader, device = load_models_and_data(args_path)

    {% for evaluator in evaluators %}
    {% include 'generator/templates/evaluators/' + evaluator + '.call.template' %}

    {% endfor %}


if __name__ == "__main__":
    main()